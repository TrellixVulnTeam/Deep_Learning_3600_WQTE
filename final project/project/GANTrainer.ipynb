{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcc3C3ZnK0OD"
      },
      "source": [
        "$$\n",
        "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
        "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
        "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
        "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
        "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
        "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
        "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
        "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
        "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
        "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
        "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
        "\\newcommand{\\bm}[1]{{\\bf #1}}\n",
        "\\newcommand{\\bb}[1]{\\bm{\\mathrm{#1}}}\n",
        "$$\n",
        "\n",
        "#GAN project models\n",
        "<a id=part3></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3O98tM6K0OI"
      },
      "source": [
        "In this part we will implement and train a generative adversarial network and apply it to the task of image generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "sj6RhUhfK0OI"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "import urllib\n",
        "import shutil\n",
        "import re\n",
        "import zipfile\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "test = unittest.TestCase()\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload"
      ],
      "metadata": {
        "id": "6ZofYig6w7he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "Q16wEtg1Ldya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Joel's path in drive:"
      ],
      "metadata": {
        "id": "VFz1uRG76ycJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/IDC/Courses/hw4 - pro"
      ],
      "metadata": {
        "id": "LRsOHThKNzbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gil's path in drive:"
      ],
      "metadata": {
        "id": "gjvoKmkJ63wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/finalproject_mlds_MSc/hw4 - project/hw4 - pro"
      ],
      "metadata": {
        "id": "CH0Gco_cAyGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from project.inception import inception_score"
      ],
      "metadata": {
        "id": "X8RDVRCq1As9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mxXaKy_K0OL"
      },
      "source": [
        "### Obtaining the dataset\n",
        "<a id=part3_1></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKsbc8k1K0OL"
      },
      "source": [
        "We'll use the same data as in Part 2.\n",
        "\n",
        "But again, you can use a custom dataset, by editing the `PART3_CUSTOM_DATA_URL` variable in `hw4/answers.py`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from project.gan_models import *"
      ],
      "metadata": {
        "id": "9h0_2QWMBfsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_gwb = load_bush_dataset()"
      ],
      "metadata": {
        "id": "cH9QNUH7Bktt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "xqubVyRkK0OW"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import project.gan_models as gan\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "def prepare_trainer(dl_train, im_size, z_dim, hp, sn=False, wgan=False):\n",
        "  # Model\n",
        "  dsc = gan.Discriminator(im_size, sn).to(device)\n",
        "  gen = gan.Generator(z_dim, featuremap_size=4).to(device)\n",
        "\n",
        "  # Optimizer\n",
        "  def create_optimizer(model_params, opt_params):\n",
        "      opt_params = opt_params.copy()\n",
        "      optimizer_type = opt_params['type']\n",
        "      opt_params.pop('type')\n",
        "      return optim.__dict__[optimizer_type](model_params, **opt_params)\n",
        "  dsc_optimizer = create_optimizer(dsc.parameters(), hp['discriminator_optimizer'])\n",
        "  gen_optimizer = create_optimizer(gen.parameters(), hp['generator_optimizer'])\n",
        "\n",
        "  # Training\n",
        "  name = ''\n",
        "  name += 'wgan' if wgan==True else 'gan'\n",
        "  name += '_sn' if sn==True else ''\n",
        "  name += f\"_ncritic_{str(hp['n_critic'])}\" if wgan==True else ''\n",
        "  checkpoint_file = f'checkpoints/{name}'\n",
        "  checkpoint_file_final = f'{checkpoint_file}_final'\n",
        "  if os.path.isfile(f'{checkpoint_file}.pt'):\n",
        "      os.remove(f'{checkpoint_file}.pt')\n",
        "\n",
        "  return dsc, gen, dsc_optimizer, gen_optimizer, checkpoint_file, checkpoint_file_final, name\n",
        "\n",
        "# Loss\n",
        "def dsc_loss_fn(y_data, y_generated, wgan, hp):\n",
        "    if wgan == False:\n",
        "      return gan.discriminator_loss_fn(y_data, y_generated, hp['data_label'], hp['label_noise'])\n",
        "    else:\n",
        "      return gan.wgan_discriminator_loss_fn(y_data, y_generated)\n",
        "\n",
        "def gen_loss_fn(y_generated, wgan, hp):\n",
        "    if wgan == False:\n",
        "      return gan.generator_loss_fn(y_generated, hp['data_label'])\n",
        "    else:\n",
        "      return gan.wgan_generator_loss_fn(y_generated)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "u0nT2E84K0OX"
      },
      "outputs": [],
      "source": [
        "import IPython.display\n",
        "import tqdm\n",
        "from project.gan_models import train_batch, save_checkpoint\n",
        "\n",
        "def train_model(name, checkpoint_file, num_epochs, dl_train, dsc, gen, dsc_loss_fn, gen_loss_fn, dsc_optimizer, gen_optimizer, wgan, hp):\n",
        "  print(f'*********** TRAINING MODEL {name} WITH hyperparams> **************')\n",
        "  print(hp)\n",
        "  try:\n",
        "      dsc_avg_losses, gen_avg_losses = [], []\n",
        "      IS = {'score':[],'std':[]}\n",
        "      for epoch_idx in range(num_epochs):\n",
        "          # We'll accumulate batch losses and show an average once per epoch.\n",
        "          dsc_losses, gen_losses = [], []\n",
        "          \n",
        "          print(f'--- EPOCH {epoch_idx+1}/{num_epochs} ---')\n",
        "\n",
        "          with tqdm.tqdm(total=len(dl_train.batch_sampler), file=sys.stdout) as pbar:\n",
        "              for batch_idx, (x_data, _) in enumerate(dl_train):\n",
        "                  x_data = x_data.to(device)\n",
        "                  dsc_loss, gen_loss = train_batch(\n",
        "                      dsc, gen,\n",
        "                      dsc_loss_fn, gen_loss_fn,\n",
        "                      dsc_optimizer, gen_optimizer,\n",
        "                      x_data, wgan, hp)\n",
        "                  dsc_losses.append(dsc_loss)\n",
        "                  gen_losses.append(gen_loss)\n",
        "                  pbar.update()\n",
        "          mu,sigma = inception_score(gen.sample(1000, with_grad=False), cuda=True, batch_size=32, resize=True, splits=1)\n",
        "          IS['score'].append(mu)\n",
        "          IS['std'].append(sigma)\n",
        "          dsc_avg_losses.append(np.mean(dsc_losses))\n",
        "          gen_avg_losses.append(np.mean(gen_losses))\n",
        "          print(f'Discriminator loss: {dsc_avg_losses[-1]}')\n",
        "          print(f'Generator loss:     {gen_avg_losses[-1]}')\n",
        "          print(f'Inception Score , std are: {IS[\"score\"][-1]},{IS[\"std\"][-1]}')\n",
        "          if save_checkpoint(gen, dsc_avg_losses, gen_avg_losses, checkpoint_file):\n",
        "              print(f'Saved checkpoint.')\n",
        "              \n",
        "          if (epoch_idx+1) % 50 == 0:\n",
        "            samples = gen.sample(5, with_grad=False)\n",
        "            fig, _ = plot.tensors_as_images(samples.cpu(), figsize=(6,2))\n",
        "            IPython.display.display(fig)\n",
        "            plt.close(fig)\n",
        "\n",
        "      print('\\n\\n\\n*** Images Generated from best model:')\n",
        "      samples = gen.sample(n=15, with_grad=False).cpu()\n",
        "      fig, _ = plot.tensors_as_images(samples, nrows=3, figsize=(6,6))\n",
        "      IPython.display.display(fig)\n",
        "      plt.close(fig)\n",
        "      if wgan:\n",
        "          with open(f\"{name}_{hp['batch_size']}_{hp['z_dim']}_{hp['n_critic']}.pickle\", 'wb') as handle:\n",
        "            pickle.dump(IS, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "      else:\n",
        "          with open(f\"{name}_{hp['batch_size']}_{hp['z_dim']}_{hp['label_noise']}.pickle\", 'wb') as handle:\n",
        "            pickle.dump(IS, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  except KeyboardInterrupt as e:\n",
        "      print('\\n *** Training interrupted by user')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "dataset = ds_gwb\n",
        "dl_train = DataLoader(dataset, batch_size, shuffle=True)\n",
        "im_size = dataset[0][0].shape\n",
        "\n",
        "num_epochs = 100"
      ],
      "metadata": {
        "id": "ZEVXi3votwuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from project.hyperparams import vanilla_hyperparams\n",
        "hp = vanilla_hyperparams()\n",
        "z_dim = hp['z_dim']\n",
        "sn = False\n",
        "wgan = False\n",
        "\n",
        "dsc, gen, dsc_optimizer, gen_optimizer, checkpoint_file, checkpoint_file_final, name = prepare_trainer(dl_train, im_size, z_dim, \n",
        "                                                                                                       hp, sn, wgan)\n",
        "vanilla_checkpoint_file = train_model(name, checkpoint_file, num_epochs, dl_train, dsc, gen, dsc_loss_fn, gen_loss_fn, dsc_optimizer, gen_optimizer, wgan, hp)\n"
      ],
      "metadata": {
        "id": "BFqYhkKBqabn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from project.hyperparams import sn_gan_hyperparams\n",
        "hp = sn_gan_hyperparams()\n",
        "z_dim = hp['z_dim']\n",
        "sn = True\n",
        "wgan = False\n",
        "dsc, gen, dsc_optimizer, gen_optimizer, checkpoint_file, checkpoint_file_final, name = prepare_trainer(dl_train, im_size, z_dim, \n",
        "                                                                                                      hp, sn, wgan)\n",
        "sn_gan_checkpoint_file = train_model(name, checkpoint_file, num_epochs, dl_train, dsc, gen, dsc_loss_fn, gen_loss_fn, dsc_optimizer, gen_optimizer, wgan, hp)\n"
      ],
      "metadata": {
        "id": "yh0mOv_ZpSTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from project.hyperparams import wgan_hyperparams\n",
        "hp = wgan_hyperparams()\n",
        "z_dim = hp['z_dim']\n",
        "for nc in [1,2,5,10,20]:\n",
        "  hp['n_critic'] = nc\n",
        "  sn = False\n",
        "  wgan = True\n",
        "  dsc, gen, dsc_optimizer, gen_optimizer, checkpoint_file, checkpoint_file_final, name = prepare_trainer(dl_train, im_size, z_dim, \n",
        "                                                                                                        hp, sn, wgan)\n",
        "  sn_gan_checkpoint_file = train_model(name, checkpoint_file, num_epochs, dl_train, dsc, gen, dsc_loss_fn, gen_loss_fn, dsc_optimizer, gen_optimizer, wgan, hp)\n"
      ],
      "metadata": {
        "id": "42quI8lLwHuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from project.hyperparams import wgan_hyperparams\n",
        "hp = wgan_hyperparams()\n",
        "z_dim = hp['z_dim']\n",
        "for nc in [1,2,5,10,20]:\n",
        "  hp['n_critic'] = nc\n",
        "  sn = True\n",
        "  wgan = True\n",
        "  dsc, gen, dsc_optimizer, gen_optimizer, checkpoint_file, checkpoint_file_final, name = prepare_trainer(dl_train, im_size, z_dim, \n",
        "                                                                                                        hp, sn, wgan)\n",
        "  sn_gan_checkpoint_file = train_model(name, checkpoint_file, num_epochs, dl_train, dsc, gen, dsc_loss_fn, gen_loss_fn, dsc_optimizer, gen_optimizer, wgan, hp)"
      ],
      "metadata": {
        "id": "Pitmbuu710u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from project.hyperparams import wgan_hyperparams\n",
        "hp = wgan_hyperparams()\n",
        "z_dim = hp['z_dim']\n",
        "hp['n_critic'] = 5\n",
        "sn = True\n",
        "wgan = True\n",
        "dsc, gen, dsc_optimizer, gen_optimizer, checkpoint_file, checkpoint_file_final, name = prepare_trainer(dl_train, im_size, z_dim, \n",
        "                                                                                                      hp, sn, wgan)\n",
        "sn_gan_checkpoint_file = train_model(name, checkpoint_file, num_epochs, dl_train, dsc, gen, dsc_loss_fn, gen_loss_fn, dsc_optimizer, gen_optimizer, wgan, hp)\n"
      ],
      "metadata": {
        "id": "tgtZiAP5Kjbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from project.hyperparams import wgan_hyperparams\n",
        "hp = wgan_hyperparams()\n",
        "z_dim = hp['z_dim']\n",
        "hp['n_critic'] = 20\n",
        "sn = True\n",
        "wgan = True\n",
        "dsc, gen, dsc_optimizer, gen_optimizer, checkpoint_file, checkpoint_file_final, name = prepare_trainer(dl_train, im_size, z_dim, \n",
        "                                                                                                      hp, sn, wgan)\n",
        "sn_gan_checkpoint_file = train_model(name, checkpoint_file, num_epochs, dl_train, dsc, gen, dsc_loss_fn, gen_loss_fn, dsc_optimizer, gen_optimizer, wgan, hp)"
      ],
      "metadata": {
        "id": "kYau29qqKmap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vMxzqNT13_07"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "GANTrainer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}